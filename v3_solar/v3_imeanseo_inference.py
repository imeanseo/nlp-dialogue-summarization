# ========================================
# v3_inference.py (v3.5 - ë¬¸ì²´ ë³€í™˜ ì¶”ê°€)
# ========================================

import torch
import pandas as pd
import yaml
import os
import time
from tqdm import tqdm
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel, PeftConfig
import re


def detect_dialogue_style(dialogue: str) -> str:
    """
    ëŒ€í™” ë¬¸ì²´ ê°ì§€ (ë°˜ë§ vs ì¡´ëŒ“ë§)
    
    Returns:
        "formal": ì¡´ëŒ“ë§ (ìš”ì•½ë¬¸ -í•©ë‹ˆë‹¤ ì²´ ìœ ì§€)
        "informal": ë°˜ë§ (ìš”ì•½ë¬¸ -í•œë‹¤ ì²´ë¡œ ë³€í™˜)
    """
    # ì¡´ëŒ“ë§ íŒ¨í„´
    formal_patterns = [
        r'ìš”[\.\?!,]',        # ~ìš”.
        r'ìŠµë‹ˆë‹¤[\.\?!,]',    # ~ìŠµë‹ˆë‹¤.
        r'ã…‚ë‹ˆë‹¤[\.\?!,]',    # ~ã…‚ë‹ˆë‹¤.
        r'ì„¸ìš”[\.\?!,]',      # ~ì„¸ìš”.
        r'ì‹œì£ [\.\?!,]',      # ~ì‹œì£ .
        r'ì–´ìš”[\.\?!,]',      # ~ì–´ìš”.
        r'ì•„ìš”[\.\?!,]',      # ~ì•„ìš”.
        r'í•´ìš”[\.\?!,]',      # ~í•´ìš”.
        r'êµ°ìš”[\.\?!,]',      # ~êµ°ìš”.
    ]
    
    # ë°˜ë§ íŒ¨í„´
    informal_patterns = [
        r'[ê°€-í£]ë‹¤[\.\?!,]',  # ~ë‹¤.
        r'[ê°€-í£]ì•¼[\.\?!,]',  # ~ì•¼.
        r'[ê°€-í£]ì§€[\.\?!,]',  # ~ì§€.
        r'[ê°€-í£][ì–´ì•„][\.\?!,]',  # ~ì–´. ~ì•„.
        r'ë„¤[\.\?!,]',         # ~ë„¤.
        r'êµ°[\.\?!,]',         # ~êµ°.
    ]
    
    # ì¹´ìš´íŒ…
    formal_count = sum(len(re.findall(p, dialogue)) for p in formal_patterns)
    informal_count = sum(len(re.findall(p, dialogue)) for p in informal_patterns)
    
    # ì¡´ëŒ“ë§ì´ ë” ë§ê±°ë‚˜ ê°™ìœ¼ë©´ formal (ê¸°ë³¸ê°’)
    return "formal" if formal_count >= informal_count else "informal"


def convert_summary_style(summary: str, style: str) -> str:
    """
    ìš”ì•½ë¬¸ ë¬¸ì²´ ë³€í™˜
    
    Args:
        summary: ì›ë³¸ ìš”ì•½ë¬¸
        style: "formal" ë˜ëŠ” "informal"
    
    Returns:
        ë³€í™˜ëœ ìš”ì•½ë¬¸
    """
    if style == "informal":
        # -í•©ë‹ˆë‹¤/-ìŠµë‹ˆë‹¤ â†’ -í•œë‹¤/-ë‹¤ ì²´ ë³€í™˜
        conversions = [
            (r'í•©ë‹ˆë‹¤\.', 'í•œë‹¤.'),
            (r'í•©ë‹ˆë‹¤,', 'í•œë‹¤,'),
            (r'ìŠµë‹ˆë‹¤\.', 'ë‹¤.'),
            (r'ìŠµë‹ˆë‹¤,', 'ë‹¤,'),
            (r'ë©ë‹ˆë‹¤\.', 'ëœë‹¤.'),
            (r'ë©ë‹ˆë‹¤,', 'ëœë‹¤,'),
            (r'ì…ë‹ˆë‹¤\.', 'ì´ë‹¤.'),
            (r'ì…ë‹ˆë‹¤,', 'ì´ë‹¤,'),
            (r'ìˆìŠµë‹ˆë‹¤\.', 'ìˆë‹¤.'),
            (r'ìˆìŠµë‹ˆë‹¤,', 'ìˆë‹¤,'),
            (r'ì—†ìŠµë‹ˆë‹¤\.', 'ì—†ë‹¤.'),
            (r'ì—†ìŠµë‹ˆë‹¤,', 'ì—†ë‹¤,'),
            (r'ê°‘ë‹ˆë‹¤\.', 'ê°„ë‹¤.'),
            (r'ì˜µë‹ˆë‹¤\.', 'ì˜¨ë‹¤.'),
            (r'ë´…ë‹ˆë‹¤\.', 'ë³¸ë‹¤.'),
            (r'ë§Œë‚©ë‹ˆë‹¤\.', 'ë§Œë‚œë‹¤.'),
            (r'ë°›ìŠµë‹ˆë‹¤\.', 'ë°›ëŠ”ë‹¤.'),
            (r'ì¤ë‹ˆë‹¤\.', 'ì¤€ë‹¤.'),
        ]
        
        for pattern, replacement in conversions:
            summary = re.sub(pattern, replacement, summary)
    
    # formalì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    return summary


def postprocess_cleanup(text: str) -> str:
    """
    LLM ìƒì„± í…ìŠ¤íŠ¸ ì •ì œ (v3.1 ê°œì„ íŒ)
    """
    # 1. í”„ë¡¬í”„íŠ¸ ì”ì¬ ì œê±°
    remove_patterns = [
        r'###?\s*Response:?.*$',
        r'###?\s*Instruction:?.*$',
        r'###?\s*Input:?.*$',
    ]
    for pattern in remove_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # 2. ëŒ€í™”ë¬¸ ìœ ì¶œ ì œê±° (ê°•í™”íŒ)
    dialogue_pattern = r'(#Person\d+#\s*[:ï¼š])'
    match = re.search(dialogue_pattern, text)
    if match:
        text = text[:match.start()]
    
    # 3. ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'\s+', ' ', text)
    
    # 4. ëŠê¸´ ë¬¸ì¥ ì²˜ë¦¬
    text = text.strip()
    
    if text and text[-1] not in '.!?ã€‚':
        last_period = max(
            text.rfind('.'),
            text.rfind('!'),
            text.rfind('?'),
            text.rfind('ã€‚')
        )
        
        if last_period > len(text) * 0.5:
            text = text[:last_period + 1]
        elif text:
            text = text.rstrip() + '.'
    
    # 5. ë¶ˆì™„ì „í•œ ë¬¸ì ì œê±°
    text = re.sub(r'[^\w\s\.,!?;:()#\-ê°€-í£]', '', text)
    
    # 6. ë¹ˆ ë¬¸ìì—´ ì²´í¬
    text = text.strip()
    if not text or len(text) < 10:
        return "ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return text


print("=" * 60)
print("v3_inference.py - LLM Inference (v3.5 ë¬¸ì²´ ë³€í™˜)")
print("=" * 60)


# 1. Config & ì„¤ì •
with open('./v3_config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# â˜…â˜…â˜… Adapter ê²½ë¡œ ìë™ íƒìƒ‰ â˜…â˜…â˜…
adapter_path = os.path.join(config['general']['output_dir'], "final_adapter")
# adapter_path = "./checkpoints_v3_improved/checkpoint-1500"
# final_adapterê°€ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ checkpoint ì‚¬ìš©
if not os.path.exists(adapter_path):
    checkpoint_dir = config['general']['output_dir']
    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]
    if checkpoints:
        latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))[-1]
        adapter_path = os.path.join(checkpoint_dir, latest_checkpoint)
        print(f"âš ï¸ final_adapter ì—†ìŒ. ìµœì‹  checkpoint ì‚¬ìš©: {latest_checkpoint}")

print(f"ğŸ“‚ Adapter Path: {adapter_path}")


# 2. ëª¨ë¸ ë¡œë“œ
base_model_name = config['general']['model_name']

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

print(f"ğŸ¤– Base Model ë¡œë“œ ì¤‘: {base_model_name}")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

print("ğŸ”— LoRA Adapter ì—°ê²° ì¤‘...")
model = PeftModel.from_pretrained(base_model, adapter_path)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"


# 3. ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv('./processed_data_v4/test.csv')

# â˜…â˜…â˜… í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ â˜…â˜…â˜…
TEST_MODE = True  # True: 10ê°œë§Œ, False: ì „ì²´ 500ê°œ

if TEST_MODE:
    test_df = test_df.head(10)
    print("\nâš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 10ê°œë§Œ ì¶”ë¡ ")
else:
    print(f"\nğŸš€ ì „ì²´ ì¶”ë¡  ëª¨ë“œ: {len(test_df)}ê°œ")

# í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
prompts = [p.rstrip() + "\n" for p in test_df['prompt'].tolist()]

print(f"âœ… Test ë°ì´í„°: {len(prompts)}ê°œ")
print(f"ğŸ“ ì²« í”„ë¡¬í”„íŠ¸ ë ë¶€ë¶„:")
print(repr(prompts[0][-80:]))


# 4. ì¶”ë¡  ë£¨í”„
results = []
batch_size = 8

# 4. ì¶”ë¡  ë£¨í”„ ì¤‘ ë””ì½”ë”© ë¶€ë¶„
for i in tqdm(range(0, len(prompts), batch_size)):
    batch_prompts = prompts[i : i + batch_size]
    batch_dialogues = test_df.iloc[i:i+batch_size]['dialogue'].tolist()
    
    inputs = tokenizer(
        batch_prompts, 
        return_tensors="pt", 
        padding=True,           # padding ì¶”ê°€ë¨
        truncation=True, 
        max_length=1024
    ).to("cuda")
    
    if i == 0:
        print(f"\nğŸ” ì²« ë°°ì¹˜ ì •ë³´:")
        print(f"  - í† í°í™” shape: {inputs['input_ids'].shape}")
        print(f"  - Attention mask shape: {inputs['attention_mask'].shape}")
        print(f"  - ì²« ìƒ˜í”Œ ì‹¤ì œ ê¸¸ì´: {inputs['attention_mask'][0].sum().item()}")
    
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=False,
            repetition_penalty=1.2,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    elapsed = time.time() - start_time
    if i == 0:
        print(f"  - ìƒì„± ì‹œê°„: {elapsed:.1f}ì´ˆ")
        total_time = elapsed * (len(prompts) / batch_size)
        print(f"  - ì˜ˆìƒ ì „ì²´ ì‹œê°„: {total_time/60:.1f}ë¶„")
    
    # â˜…â˜…â˜… ë””ì½”ë”© ê°œì„  â˜…â˜…â˜…
    for j, output in enumerate(outputs):
        # padding ì œì™¸í•œ ì‹¤ì œ ì…ë ¥ ê¸¸ì´
        actual_input_length = inputs['attention_mask'][j].sum().item()
        
        # ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        generated_ids = output[actual_input_length:]
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        # ì²« ìƒ˜í”Œ ë””ë²„ê¹…
        if i == 0 and j == 0:
            print(f"\nğŸ“ ë””ì½”ë”© ë””ë²„ê¹…:")
            print(f"  - ì „ì²´ ì¶œë ¥ ê¸¸ì´: {len(output)} í† í°")
            print(f"  - ì‹¤ì œ ì…ë ¥ ê¸¸ì´: {actual_input_length} í† í°")
            print(f"  - ìƒì„±ëœ ê¸¸ì´: {len(generated_ids)} í† í°")
            print(f"  - ìƒì„± í…ìŠ¤íŠ¸ (ì²˜ìŒ 200ì): {generated_text[:200]}")
        
        # ì •ì œ
        summary = postprocess_cleanup(generated_text)
        
        # ë¬¸ì²´ ë³€í™˜
        dialogue_style = detect_dialogue_style(batch_dialogues[j])
        summary = convert_summary_style(summary, dialogue_style)
        
        # ì²« ìƒ˜í”Œ ìµœì¢… ê²°ê³¼
        if i == 0 and j == 0:
            print(f"\nğŸ¨ ë¬¸ì²´ ë³€í™˜:")
            print(f"  - ê°ì§€ëœ ë¬¸ì²´: {dialogue_style}")
            print(f"  - ìµœì¢… ìš”ì•½: {summary}")
        
        results.append(summary)



# 5. ì €ì¥
print("\nğŸ’¾ ì œì¶œ íŒŒì¼ ì €ì¥ ì¤‘...")
from datetime import datetime
import pytz  # ì‹œê°„ëŒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬

# â˜…â˜…â˜… í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€ê²½ â˜…â˜…â˜…
kst = pytz.timezone('Asia/Seoul')
date_str = datetime.now(kst).strftime('%m%d_%H%M') 
mode_suffix = "test10" if TEST_MODE else "full"
filename = f'submission_v4_{mode_suffix}_{date_str}.csv'

submission = pd.DataFrame({
    'fname': [f'test_{i}' for i in range(len(results))],
    'summary': results
})

os.makedirs('./predictions', exist_ok=True)
submission.to_csv(f'./predictions/{filename}', index=False)

print(f"âœ… ì™„ë£Œ! ./predictions/{filename}")

# ìƒ˜í”Œ í™•ì¸
print("\nğŸ“ ìƒì„± ìƒ˜í”Œ:")
for i in range(min(5, len(results))):
    print(f"[{i}] {results[i]}")
    print("-" * 30)
