general:
  data_path: ./
  model_name: facebook/mbart-large-50-many-to-many-mmt
  output_dir: ./checkpoints
inference:
  batch_size: 32
  early_stopping: true
  generate_max_length: 80
  no_repeat_ngram_size: 2
  num_beams: 6
  remove_tokens:
  - <usr>
  - </s>
  - <s>
  - <pad>
tokenizer:
  decoder_max_len: 80
  encoder_max_len: 400
  special_tokens:
  - '#Address#'
  - '#Alex#'
  - '#Bob#'
  - '#CarNumber#'
  - '#CardNumber#'
  - '#DateOfBirth#'
  - '#Email#'
  - '#Kristin#'
  - '#Liliana#'
  - '#Name#'
  - '#PassportNumber#'
  - '#Person1#'
  - '#Person2#'
  - '#Person3#'
  - '#Person4#'
  - '#Person5#'
  - '#Person6#'
  - '#Person7#'
  - '#PersonName#'
  - '#PhoneNumber#'
  - '#Price#'
  - '#SSN#'
training:
  do_eval: true
  do_train: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  eval_steps: 500
  fp16: true
  generation_max_length: 80
  gradient_accumulation_steps: 2
  learning_rate: 5.0e-05
  load_best_model_at_end: true
  logging_dir: ./logs
  logging_steps: 100
  logging_strategy: steps
  lr_scheduler_type: cosine
  num_train_epochs: 10
  optim: adamw_torch
  overwrite_output_dir: true
  per_device_eval_batch_size: 8
  per_device_train_batch_size: 8
  predict_with_generate: true
  report_to: wandb
  save_steps: 500
  save_total_limit: 3
  seed: 42
  warmup_ratio: 0.1
  weight_decay: 0.01
wandb:
  entity: imeanseo_
  name: v2-imeanseo-mbart
  project: dialogue-summarization
