# ========================================
# v2_train.py
# mBART Fine-tuning
# ========================================

import pandas as pd
import numpy as np
import os
import yaml
import torch
import re
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from datasets import Dataset
from rouge import Rouge, rouge
import warnings
warnings.filterwarnings('ignore')

print("=" * 60)
print("v1_train.py - mBART Fine-tuning")
print("=" * 60)

# ========================================
# 1. Config ë¶ˆëŸ¬ì˜¤ê¸°
# ========================================

print("\nğŸ“– Config ë¶ˆëŸ¬ì˜¤ê¸°...")
config_path = './v2_config.yaml'

with open(config_path, 'r', encoding='utf-8') as file:
    config = yaml.safe_load(file)

config['training']['learning_rate'] = float(config['training']['learning_rate'])
config['training']['num_train_epochs'] = int(config['training']['num_train_epochs'])
config['training']['per_device_train_batch_size'] = int(config['training']['per_device_train_batch_size'])
config['training']['per_device_eval_batch_size'] = int(config['training']['per_device_eval_batch_size'])
config['training']['warmup_ratio'] = float(config['training']['warmup_ratio'])
config['training']['weight_decay'] = float(config['training']['weight_decay'])
config['training']['gradient_accumulation_steps'] = int(config['training']['gradient_accumulation_steps'])
config['training']['save_steps'] = int(config['training']['save_steps'])
config['training']['eval_steps'] = int(config['training']['eval_steps'])
config['training']['save_total_limit'] = int(config['training']['save_total_limit'])
config['training']['seed'] = int(config['training']['seed'])
config['training']['logging_steps'] = int(config['training']['logging_steps'])
config['training']['generation_max_length'] = int(config['training']['generation_max_length'])
config['training']['early_stopping_patience'] = int(config['training']['early_stopping_patience'])
config['training']['early_stopping_threshold'] = float(config['training']['early_stopping_threshold'])
config['tokenizer']['encoder_max_len'] = int(config['tokenizer']['encoder_max_len'])
config['tokenizer']['decoder_max_len'] = int(config['tokenizer']['decoder_max_len'])

print("âœ… Config ë¡œë“œ ì™„ë£Œ")
print(f"  Model: {config['general']['model_name']}")
print(f"  Encoder Max: {config['tokenizer']['encoder_max_len']}")
print(f"  Decoder Max: {config['tokenizer']['decoder_max_len']}")
print(f"  Batch Size: {config['training']['per_device_train_batch_size']}")
print(f"  Learning Rate: {config['training']['learning_rate']}")
print(f"  Epochs: {config['training']['num_train_epochs']}")

# ========================================
# 2. GPU í™•ì¸
# ========================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nğŸ–¥ï¸ Device: {device}")

if torch.cuda.is_available():
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("  âš ï¸ GPU ì—†ìŒ - CPUë¡œ í•™ìŠµ (ë§¤ìš° ëŠë¦¼)")

# ========================================
# 3. WandB ì´ˆê¸°í™”
# ========================================

try:
    import wandb
    
    print("\nğŸ”— WandB ì´ˆê¸°í™”...")
    
    # WandB ë¡œê·¸ì¸ ì²´í¬
    try:
        wandb.login()
    except:
        print("âš ï¸ WandB ë¡œê·¸ì¸ í•„ìš”: wandb login")
    
    # ì´ˆê¸°í™”
    wandb.init(
        entity=config['wandb']['entity'],
        project=config['wandb']['project'],
        name=config['wandb']['name'],
        config=config,
        resume='allow',  # ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê°€ëŠ¥
    )
    
    # Baseline ìŠ¤íƒ€ì¼ í™˜ê²½ë³€ìˆ˜
    os.environ["WANDB_LOG_MODEL"] = "end"
    os.environ["WANDB_WATCH"] = "false"
    
    print(f"âœ… WandB ì´ˆê¸°í™” ì™„ë£Œ")
    print(f"ğŸ“Š Dashboard: {wandb.run.get_url()}")
    use_wandb = True
    
except Exception as e:
    print(f"\nâš ï¸ WandB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    print("WandB ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    use_wandb = False
    config['training']['report_to'] = 'none'

# ========================================
# 4. ë°ì´í„° ë¡œë“œ (ì „ì²˜ë¦¬ëœ ë°ì´í„°)
# ========================================

print("\nğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©...")

train_df = pd.read_csv('./processed_data/train_processed.csv')
dev_df = pd.read_csv('./processed_data/dev_processed.csv')

print(f"âœ… Train: {len(train_df):,}ê°œ")
print(f"âœ… Dev:   {len(dev_df):,}ê°œ")

# ========================================
# 5. í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ
# ========================================

print(f"\nğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë“œ: {config['general']['model_name']}")

tokenizer = AutoTokenizer.from_pretrained(
    config['general']['model_name'],
    src_lang='ko_KR',
    tgt_lang='ko_KR'
)

# Special Token ì¶”ê°€
special_tokens = config['tokenizer']['special_tokens']
num_added = tokenizer.add_special_tokens({
    'additional_special_tokens': special_tokens
})
print(f"âœ… {num_added}ê°œ Special Token ì¶”ê°€")

# ì²´í¬í¬ì¸íŠ¸ í™•ì¸
import glob
checkpoints = glob.glob(os.path.join(config['general']['output_dir'], "checkpoint-*"))
last_checkpoint = None

if checkpoints:
    checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))
    last_checkpoint = checkpoints[-1]
    print(f"\nğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {last_checkpoint}")

# ëª¨ë¸ ë¡œë“œ
print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ...")
if last_checkpoint:
    print(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {last_checkpoint}")
    model = AutoModelForSeq2SeqLM.from_pretrained(last_checkpoint)
else:
    print(f"ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ: {config['general']['model_name']}")
    model = AutoModelForSeq2SeqLM.from_pretrained(config['general']['model_name'])

# Vocab í¬ê¸° ì¡°ì •
model.resize_token_embeddings(len(tokenizer))
print(f"âœ… Vocab í¬ê¸°: {len(tokenizer)}")

# GPUë¡œ ì´ë™
model.to(device)

# ========================================
# 6. Dataset ì¤€ë¹„
# ========================================

print("\nğŸ“Š Dataset ìƒì„±...")

def preprocess_function(examples):
    """í† í¬ë‚˜ì´ì§• í•¨ìˆ˜"""
    
    # ì…ë ¥ (ëŒ€í™”ë¬¸)
    inputs = examples['dialogue_clean']
    targets = examples['summary_clean']
    
    # í† í¬ë‚˜ì´ì§•
    model_inputs = tokenizer(
        inputs,
        max_length=config['tokenizer']['encoder_max_len'],
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # íƒ€ê²Ÿ í† í¬ë‚˜ì´ì§•
    labels = tokenizer(
        targets,
        max_length=config['tokenizer']['decoder_max_len'],
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    model_inputs['labels'] = labels['input_ids']
    
    return model_inputs

# HuggingFace Datasetìœ¼ë¡œ ë³€í™˜
train_dataset = Dataset.from_pandas(train_df[['dialogue_clean', 'summary_clean']])
val_dataset = Dataset.from_pandas(dev_df[['dialogue_clean', 'summary_clean']])

# í† í¬ë‚˜ì´ì§• ì ìš©
print("í† í¬ë‚˜ì´ì§• ì¤‘...")
train_dataset = train_dataset.map(
    preprocess_function, 
    batched=True, 
    remove_columns=['dialogue_clean', 'summary_clean']
)
val_dataset = val_dataset.map(
    preprocess_function, 
    batched=True, 
    remove_columns=['dialogue_clean', 'summary_clean']
)

print(f"âœ… Train Dataset: {len(train_dataset):,}ê°œ")
print(f"âœ… Val Dataset: {len(val_dataset):,}ê°œ")

# Data Collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True
)

# ========================================
# 7. ROUGE í‰ê°€ í•¨ìˆ˜ ìˆ˜ì •
# ========================================

def compute_metrics(eval_pred):
    """ROUGE ì ìˆ˜ ê³„ì‚°"""
    predictions, labels = eval_pred
    
    # -100ì„ íŒ¨ë”©ìœ¼ë¡œ ì²˜ë¦¬
    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    
    # ë””ì½”ë”©
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=False)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)
    
    rouge_scorer = Rouge()
    
    def clean_text(text):
        """ëª¨ë¸ í† í°ë§Œ ì œê±°, #Person#ì€ ìœ ì§€"""
        # 1) ëª¨ë¸ íŠ¹ìˆ˜ í† í° ì œê±°
        remove_tokens = ['<usr>', '</s>', '<s>', '<pad>', '<unk>', 
                        'ko_KR', 'en_XX', 'ja_XX', 'zh_CN', '__', 'â–']
        for token in remove_tokens:
            text = text.replace(token, '')
        
        # 2) ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 3) ì£¼ì–´ ëˆ„ë½ ìˆ˜ì • 
        # "ëŠ” ì—ê²Œ" â†’ "#Person1#ëŠ” #Person2#ì—ê²Œ"
        if text.startswith('ëŠ” '):
            text = '#Person1#' + text
        text = re.sub(r'([.!?])\s+ëŠ”\s', r'\1 #Person1#ëŠ” ', text)
        text = re.sub(r'(\w+)ëŠ”\s+ì—ê²Œ', r'\1ëŠ” #Person2#ì—ê²Œ', text)
        
        return text if text else "empty"
    
    # í›„ì²˜ë¦¬ ì ìš©
    replaced_preds = [clean_text(p) for p in decoded_preds]
    replaced_labels = [clean_text(l) for l in decoded_labels]
    
    # ROUGE ê³„ì‚°
    try:
        results = rouge_scorer.get_scores(replaced_preds, replaced_labels, avg=True)
        
        # ìƒ˜í”Œ ì¶œë ¥
        print("\n" + "-" * 60)
        print(f"[ì˜ˆì¸¡] {replaced_preds[0]}")
        print(f"[ì •ë‹µ] {replaced_labels[0]}")
        print("-" * 60)
        
        return {
            'rouge1': results['rouge-1']['f'],
            'rouge2': results['rouge-2']['f'],
            'rougeL': results['rouge-l']['f'],
        }
    except Exception as e:
        print(f"âš ï¸ ROUGE ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}


# ========================================
# 8. Training Arguments
# ========================================

print("\nâš™ï¸ Training Arguments ì„¤ì •...")

training_args = Seq2SeqTrainingArguments(
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir=config['general']['output_dir'],
    
    # í•™ìŠµ (ì§ì ‘ íƒ€ì… ë³€í™˜!)
    num_train_epochs=int(config['training']['num_train_epochs']),
    per_device_train_batch_size=int(config['training']['per_device_train_batch_size']),
    per_device_eval_batch_size=int(config['training']['per_device_eval_batch_size']),
    gradient_accumulation_steps=int(config['training']['gradient_accumulation_steps']),
    
    # ìµœì í™” (ì§ì ‘ íƒ€ì… ë³€í™˜!)
    learning_rate=float(config['training']['learning_rate']),
    weight_decay=float(config['training']['weight_decay']),
    warmup_ratio=float(config['training']['warmup_ratio']),
    lr_scheduler_type=str(config['training']['lr_scheduler_type']),
    optim=str(config['training']['optim']),
    
    # í‰ê°€
    eval_strategy='steps',
    eval_steps=int(config['training']['eval_steps']),
    
    # ì²´í¬í¬ì¸íŠ¸
    save_strategy='steps',
    save_steps=int(config['training']['save_steps']),
    save_total_limit=int(config['training']['save_total_limit']),
    load_best_model_at_end=bool(config['training']['load_best_model_at_end']),
    metric_for_best_model='rougeL',
    greater_is_better=True,
    
    # ìƒì„±
    predict_with_generate=True,
    generation_max_length=int(config['tokenizer']['decoder_max_len']),
    
    # íš¨ìœ¨ì„±
    fp16=bool(config['training']['fp16']),
    
    # ë¡œê¹…
    logging_dir=str(config['training']['logging_dir']),
    logging_strategy='steps',
    logging_steps=int(config['training']['logging_steps']),
    report_to=str(config['training']['report_to']),
    
    # ê¸°íƒ€
    seed=int(config['training']['seed']),
    overwrite_output_dir=bool(config['training']['overwrite_output_dir']),
    do_train=bool(config['training']['do_train']),
    do_eval=bool(config['training']['do_eval']),
    ignore_data_skip=False,
)

print("âœ… Training Arguments ì¤€ë¹„ ì™„ë£Œ")

# ========================================
# 9. Trainer ìƒì„±
# ========================================

print("\nğŸ¯ Trainer ìƒì„±...")

# Early Stopping Callback
early_stopping = EarlyStoppingCallback(
    early_stopping_patience=config['training']['early_stopping_patience'],
    early_stopping_threshold=config['training']['early_stopping_threshold']
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

print("âœ… Trainer ì¤€ë¹„ ì™„ë£Œ")

# ========================================
# 10. í•™ìŠµ ì‹œì‘
# ========================================

print("\n" + "=" * 60)
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("=" * 60)
print(f"  Epochs: {config['training']['num_train_epochs']}")
print(f"  Batch Size: {config['training']['per_device_train_batch_size']}")
print(f"  Gradient Accumulation: {config['training']['gradient_accumulation_steps']}")
print(f"  Effective Batch: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}")
print(f"  Learning Rate: {config['training']['learning_rate']}")
print(f"  Save Steps: {config['training']['save_steps']}")
print("=" * 60)

try:
    # ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ
    if last_checkpoint:
        print(f"\nğŸ“¥ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {last_checkpoint}")
        trainer.train(resume_from_checkpoint=last_checkpoint)
    else:
        print("\nğŸ†• ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œì‘")
        trainer.train()
    
    print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
    
    # Best ëª¨ë¸ ì €ì¥
    best_model_path = os.path.join(config['general']['output_dir'], 'best_model')
    trainer.save_model(best_model_path)
    tokenizer.save_pretrained(best_model_path)
    print(f"\nğŸ’¾ Best ëª¨ë¸ ì €ì¥: {best_model_path}")
    
    # WandBì— ëª¨ë¸ ì—…ë¡œë“œ
    if use_wandb:
        artifact = wandb.Artifact('mbart-best-model', type='model')
        artifact.add_dir(best_model_path)
        wandb.log_artifact(artifact)
        print("ğŸ“¤ WandBì— ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ")

except KeyboardInterrupt:
    print("\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ (Ctrl+C)")
    print("ğŸ’¾ í˜„ì¬ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œë©ë‹ˆë‹¤.")

except Exception as e:
    print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()

finally:
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    # WandB ì¢…ë£Œ
    if use_wandb:
        wandb.finish()

# ========================================
# ì™„ë£Œ
# ========================================

print("\n" + "=" * 60)
print("âœ… v1_train.py ì™„ë£Œ!")
print("=" * 60)
print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
print(f"  - {config['general']['output_dir']}/best_model/")
print(f"  - {config['general']['output_dir']}/checkpoint-*/")
print(f"  - {config['training']['logging_dir']}/")

if use_wandb and wandb.run is not None:
    print(f"\nğŸ“Š WandB Dashboard:")
    print(f"  {wandb.run.get_url()}")

print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print("  python v1_inference.py  # ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±")
print("=" * 60)
