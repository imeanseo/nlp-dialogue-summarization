import os
import yaml
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer

print("=" * 80)
print("v5_processing.py - T5-Large ë°ì´í„° ì „ì²˜ë¦¬")
print("=" * 80)

# Config ë¡œë“œ
with open('v5_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

print(f"\nâœ… Config ë¡œë“œ ì™„ë£Œ")
print(f"  Model: {config['general']['model_name']}")
print(f"  Encoder Max Length: {config['tokenizer']['encoder_max_len']}")
print(f"  Decoder Max Length: {config['tokenizer']['decoder_max_len']}")

# ë°ì´í„° ë¡œë“œ (í´ë¦¬ë‹ëœ íŒŒì¼ ì‚¬ìš©!)
print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
train_df = pd.read_csv(os.path.join(config['general']['data_path'], 'train_cleaned.csv'))
dev_df = pd.read_csv(os.path.join(config['general']['data_path'], 'dev.csv'))
test_df = pd.read_csv(os.path.join(config['general']['data_path'], 'test.csv'))

print(f"  Train: {len(train_df)}ê°œ")
print(f"  Dev: {len(dev_df)}ê°œ")
print(f"  Test: {len(test_df)}ê°œ")

# Tokenizer ë¡œë“œ
print(f"\nğŸ”§ Tokenizer ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(
    config['general']['model_name'],
    cache_dir=config['general'].get('cache_dir', None)
)
print(f"  Vocab size: {tokenizer.vocab_size}")

# T5ëŠ” prefixë¥¼ ì‚¬ìš©í•¨
# ì˜ˆ: "summarize: <ëŒ€í™”ë¬¸>" â†’ "<ìš”ì•½ë¬¸>"
def preprocess_function(examples):
    """
    T5 ì „ìš© ì „ì²˜ë¦¬
    - Input: "summarize: <dialogue>"
    - Target: "<summary>"
    """
    # T5ëŠ” task prefixë¥¼ ì‚¬ìš©
    inputs = ["summarize: " + doc for doc in examples['dialogue']]
    targets = examples['summary']
    
    # Tokenize inputs
    model_inputs = tokenizer(
        inputs,
        max_length=config['tokenizer']['encoder_max_len'],
        padding='max_length',
        truncation=True
    )
    
    # Tokenize targets (labels)
    labels = tokenizer(
        targets,
        max_length=config['tokenizer']['decoder_max_len'],
        padding='max_length',
        truncation=True
    )
    
    model_inputs['labels'] = labels['input_ids']
    
    return model_inputs

# Dataset ë³€í™˜
print(f"\nğŸ”„ Dataset ë³€í™˜ ì¤‘...")
train_dataset = Dataset.from_pandas(train_df[['dialogue', 'summary']])
eval_dataset = Dataset.from_pandas(dev_df[['dialogue', 'summary']])
test_dataset = Dataset.from_pandas(test_df[['dialogue']])

# Tokenization ì ìš©
print(f"\nâš™ï¸ Tokenization ì ìš© ì¤‘...")
train_dataset = train_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=['dialogue', 'summary'],
    desc="Tokenizing train"
)

eval_dataset = eval_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=['dialogue', 'summary'],
    desc="Tokenizing dev"
)

# TestëŠ” labels ì—†ìŒ
def preprocess_test(examples):
    inputs = ["summarize: " + doc for doc in examples['dialogue']]
    return tokenizer(
        inputs,
        max_length=config['tokenizer']['encoder_max_len'],
        padding='max_length',
        truncation=True
    )

test_dataset = test_dataset.map(
    preprocess_test,
    batched=True,
    remove_columns=['dialogue'],
    desc="Tokenizing test"
)

# ì €ì¥
output_dir = os.path.join(config['general']['data_path'], 'processed_data_v5')
os.makedirs(output_dir, exist_ok=True)

print(f"\nğŸ’¾ ì €ì¥ ì¤‘...")
train_dataset.save_to_disk(os.path.join(output_dir, 'train'))
eval_dataset.save_to_disk(os.path.join(output_dir, 'eval'))
test_dataset.save_to_disk(os.path.join(output_dir, 'test'))

print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
print(f"  ì €ì¥ ìœ„ì¹˜: {output_dir}/")
print(f"  - train: {len(train_dataset)}ê°œ")
print(f"  - eval: {len(eval_dataset)}ê°œ")
print(f"  - test: {len(test_dataset)}ê°œ")

# ìƒ˜í”Œ í™•ì¸
print(f"\nğŸ“ ìƒ˜í”Œ í™•ì¸ (ì²« ë²ˆì§¸ ë°ì´í„°):")
sample = train_dataset[0]
print(f"  Input IDs ê¸¸ì´: {len(sample['input_ids'])}")
print(f"  Labels ê¸¸ì´: {len(sample['labels'])}")
print(f"  ì‹¤ì œ í…ìŠ¤íŠ¸ (ë””ì½”ë”©):")
print(f"    Input: {tokenizer.decode(sample['input_ids'][:100])}...")
print(f"    Label: {tokenizer.decode([id for id in sample['labels'] if id != -100][:50])}...")
