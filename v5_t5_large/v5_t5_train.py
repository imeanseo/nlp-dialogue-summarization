import os
import yaml
import wandb
from datasets import load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)

print("=" * 80)
print("v5_train.py - T5-Large í•™ìŠµ")
print("=" * 80)

# Config ë¡œë“œ
with open('v5_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

print(f"\nâœ… Config ë¡œë“œ ì™„ë£Œ")
print(f"  Model: {config['general']['model_name']}")

# WandB ì´ˆê¸°í™”
wandb.init(
    entity=config['wandb']['entity'],
    project=config['wandb']['project'],
    name=config['wandb']['name'],
    config=config
)
print(f"\nâœ… WandB ì´ˆê¸°í™” ì™„ë£Œ")

# ë°ì´í„° ë¡œë“œ
print(f"\nğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì¤‘...")
data_dir = os.path.join(config['general']['data_path'], 'processed_data_v5')
train_dataset = load_from_disk(os.path.join(data_dir, 'train'))
eval_dataset = load_from_disk(os.path.join(data_dir, 'eval'))

print(f"  Train: {len(train_dataset)}ê°œ")
print(f"  Eval: {len(eval_dataset)}ê°œ")

# Tokenizer & Model ë¡œë“œ
print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(
    config['general']['model_name'],
    cache_dir=config['general'].get('cache_dir', None)
)

model = AutoModelForSeq2SeqLM.from_pretrained(
    config['general']['model_name'],
    cache_dir=config['general'].get('cache_dir', None)
)

print(f"  ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}")

# Data Collator (T5ìš©)
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True
)

# Training Arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=config['general']['output_dir'],
    num_train_epochs=config['training']['num_train_epochs'],
    per_device_train_batch_size=config['training']['per_device_train_batch_size'],
    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],
    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
    
    learning_rate=config['training']['learning_rate'],
    warmup_steps=config['training']['warmup_steps'],
    weight_decay=config['training']['weight_decay'],
    
    save_strategy=config['training']['save_strategy'],
    eval_strategy=config['training']['evaluation_strategy'],
    save_total_limit=config['training']['save_total_limit'],
    load_best_model_at_end=config['training']['load_best_model_at_end'],
    metric_for_best_model=config['training']['metric_for_best_model'],
    
    logging_dir=config['training']['logging_dir'],
    logging_steps=config['training']['logging_steps'],
    report_to=config['training']['report_to'],
    
    fp16=config['training']['fp16'],
    gradient_checkpointing=config['training']['gradient_checkpointing'],
    dataloader_num_workers=config['training']['dataloader_num_workers'],
    
    # T5 generation ì„¤ì •
    predict_with_generate=config['training']['predict_with_generate'],
    generation_max_length=config['training']['generation_max_length'],
    generation_num_beams=config['training']['generation_num_beams'],
)

# Trainer ìƒì„±
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

print(f"\nğŸš€ í•™ìŠµ ì‹œì‘!")
print(f"  ì´ ì—í­: {config['training']['num_train_epochs']}")
print(f"  ì‹¤ì§ˆì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}")
print(f"  ì´ ìŠ¤í…: {len(train_dataset) // (config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']) * config['training']['num_train_epochs']}")

# í•™ìŠµ ì‹¤í–‰
trainer.train()

# ìµœì¢… ëª¨ë¸ ì €ì¥
final_model_path = os.path.join(config['general']['output_dir'], "final_model")
print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
trainer.save_model(final_model_path)
tokenizer.save_pretrained(final_model_path)

print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
print(f"  ì €ì¥ ìœ„ì¹˜: {final_model_path}")

wandb.finish()
