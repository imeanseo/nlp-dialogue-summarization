# ========================================
# inference.py
# mBART Inference & Submission ìƒì„±
# ========================================

import pandas as pd
import numpy as np
import os
import yaml
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("=" * 60)
print("v2_inference.py - Inference & Submission")
print("=" * 60)

# ========================================
# 1. Config ë¶ˆëŸ¬ì˜¤ê¸°
# ========================================

print("\nğŸ“– Config ë¶ˆëŸ¬ì˜¤ê¸°...")
config_path = './v2_config.yaml'

with open(config_path, 'r', encoding='utf-8') as file:
    config = yaml.safe_load(file)

print("âœ… Config ë¡œë“œ ì™„ë£Œ")

# ========================================
# 2. GPU í™•ì¸
# ========================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nğŸ–¥ï¸ Device: {device}")

if torch.cuda.is_available():
    print(f"  GPU: {torch.cuda.get_device_name(0)}")

# ========================================
# 3. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
# ========================================

print("\nğŸ¤– Best ëª¨ë¸ ë¡œë“œ...")

model_path = os.path.join(config['general']['output_dir'], 'best_model')

# ëª¨ë¸ ê²½ë¡œ í™•ì¸
if not os.path.exists(model_path):
    print(f"âš ï¸ Best model ì—†ìŒ: {model_path}")
    # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
    import glob
    checkpoints = glob.glob(os.path.join(config['general']['output_dir'], "checkpoint-*"))
    if checkpoints:
        checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))
        model_path = checkpoints[-1]
        print(f"ğŸ“¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©: {model_path}")
    else:
        print("âŒ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")
        exit(1)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    src_lang='ko_KR',
    tgt_lang='ko_KR'
)

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
model.to(device)
model.eval()

print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")


# ========================================
# 4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
# ========================================

print("\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")

# ìˆ˜ì •ëœ test_fixed.csv ì‚¬ìš©
import os
if os.path.exists('./test_fixed.csv'):
    test_df = pd.read_csv('./test_fixed.csv')
    print(f"âœ… Test (fixed): {len(test_df):,}ê°œ")
else:
    test_df = pd.read_csv('./test.csv')
    print(f"âœ… Test (original): {len(test_df):,}ê°œ")

# fname í™•ì¸
print(f"   fname ë²”ìœ„: {test_df['fname'].iloc[0]} ~ {test_df['fname'].iloc[-1]}")

# fname ìˆœì„œëŒ€ë¡œ ì •ë ¬
test_df['fname_num'] = test_df['fname'].str.extract('(\d+)').astype(int)
test_df = test_df.sort_values('fname_num').reset_index(drop=True)

# ì „ì²˜ë¦¬
import re

def clean_text(text):
    if pd.isna(text):
        return ""
    text = str(text)
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

test_df['dialogue_clean'] = test_df['dialogue'].apply(clean_text)

print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(test_df):,}ê°œ")

# ========================================
# 5. Inference í•¨ìˆ˜
# ========================================

print("\nğŸ”® Inference í•¨ìˆ˜ ì¤€ë¹„...")

def generate_summary(dialogue, model, tokenizer, device, config):
    """ë‹¨ì¼ ëŒ€í™”ë¬¸ ìš”ì•½ ìƒì„±"""

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(
        dialogue,
        max_length=config['tokenizer']['encoder_max_len'],
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    ).to(device)

    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=config['tokenizer']['decoder_max_len'],
            num_beams=config['inference']['num_beams'],
            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
            early_stopping=config['inference']['early_stopping'],
        )

    # ë””ì½”ë”©
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # í›„ì²˜ë¦¬: remove_tokens ì œê±°
    remove_tokens = config['inference']['remove_tokens']
    for token in remove_tokens:
        summary = summary.replace(token, '')

    # fix_missing_subjects í˜¸ì¶œí•˜ì—¬ ì£¼ì–´ ëˆ„ë½ ë³´ì™„
    summary = fix_missing_subjects(summary)

    # ìµœì¢… ê³µë°± ë“± ì •ë¦¬
    summary = summary.replace('\t', '').replace('  ', ' ').strip()

    return summary

print("âœ… Inference í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ")

import re

def fix_missing_subjects(summary: str) -> str:
    """ì •êµí•œ ì£¼ì–´ ë³´ì™„ - ê³¼ë„í•œ ë°˜ë³µ ë°©ì§€"""

    # 0) ì—°ì†ëœ #Person íƒœê·¸ ì¶•ì•½
    summary = re.sub(r'(#Person\d+#)+', r'#Person1#', summary)

    # 1) ë¬¸ì¥ ì‹œì‘ì´ ì¡°ì‚¬(ì€/ëŠ”/ì´/ê°€/ì—ê²Œ/ê³¼/ì™€ ë“±)ë¡œ ì‹œì‘í•˜ë©´ #Person1# ë¶™ì´ê¸°
    if re.match(r'^[ì€ëŠ”ì´ì´ê°€ì„ë¥¼ì—ê²Œê³¼ì™€]', summary):
        summary = '#Person1#' + summary

    # 2) ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ/ë¬¼ìŒí‘œ ë’¤ì— ì¡°ì‚¬ë§Œ ì˜¤ëŠ” ê²½ìš° â†’ #Person1# ë¶™ì´ê¸°
    summary = re.sub(
        r'([.!?])\s*([ì€ëŠ”ì´ì´ê°€ì„ë¥¼ì—ê²Œê³¼ì™€])',
        r'\1 #Person1#\2',
        summary
    )

    # 3) " , ê°€/ëŠ”/ì€/ì„" íŒ¨í„´ ë³´ì™„
    # ì• ë¬¸ì¥ì— #Person1#/ #Person2#ê°€ ìˆìœ¼ë©´, ì—†ë˜ ìª½ì„ ì±„ì›Œ ë„£ê¸°
    if "ê°€ " in summary or "ëŠ” " in summary or "ì€ " in summary or "ì„ " in summary:
        if "#Person1#" in summary and "#Person2#" not in summary:
            summary = summary.replace(" ê°€ ", " #Person1#ê°€ ")
            summary = summary.replace(" ëŠ” ", " #Person1#ëŠ” ")
            summary = summary.replace(" ì€ ", " #Person1#ì€ ")
            summary = summary.replace(" ì„ ", " #Person1#ì„ ")
        elif "#Person2#" in summary and "#Person1#" not in summary:
            summary = summary.replace(" ê°€ ", " #Person2#ê°€ ")
            summary = summary.replace(" ëŠ” ", " #Person2#ëŠ” ")
            summary = summary.replace(" ì€ ", " #Person2#ì€ ")
            summary = summary.replace(" ì„ ", " #Person2#ì„ ")

    # 4) "ì˜ ~" ì•ì— ì£¼ì–´ ë³´ì™„ (ëŒ€ë¶€ë¶„ #Person2#)
    summary = summary.replace(" ì˜ ì•„íŒŒíŠ¸", " #Person2#ì˜ ì•„íŒŒíŠ¸")
    summary = summary.replace(" ì˜ ê°€ë°©", " #Person2#ì˜ ê°€ë°©")
    summary = summary.replace(" ì˜ ì§‘", " #Person2#ì˜ ì§‘")
    summary = summary.replace(" ì˜ ìë™ì°¨", " #Person2#ì˜ ìë™ì°¨")
    summary = summary.replace(" ì˜ íœ´ëŒ€í°", " #Person2#ì˜ íœ´ëŒ€í°")
    summary = summary.replace(" ì˜ ê°€ê²Œ", " #Person2#ì˜ ê°€ê²Œ")
    summary = summary.replace(" ì˜ ì²­êµ¬ì„œ", " #Person2#ì˜ ì²­êµ¬ì„œ")
    summary = summary.replace(" ì˜ ì°¨", " #Person2#ì˜ ì°¨")
    summary = summary.replace(" ì˜ ë°©ë¬¸", " #Person2#ì˜ ë°©ë¬¸")

    # 5) â€œê³¼/ì™€ ëŠ”â€ íŒ¨í„´ â†’ ë’¤ ì‚¬ëŒì„ #Person2#ë¡œ ê°€ì •
    # ì˜ˆ: "#Person1#ê³¼ ëŠ”" â†’ "#Person1#ê³¼ #Person2#ëŠ”"
    summary = re.sub(
        r'(#Person1#)\s*(ê³¼|ì™€)\s+ëŠ”',
        r'\1\2 #Person2#ëŠ”',
        summary
    )

    # 6) "ì—ê²Œ" ì•ì— ì•„ë¬´ ê²ƒë„ ì—†ê±°ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° â†’ #Person2#ì—ê²Œ
    summary = re.sub(
        r'\sì—ê²Œ',
        ' #Person2#ì—ê²Œ',
        summary
    )
    summary = re.sub(
        r'^ì—ê²Œ',
        '#Person2#ì—ê²Œ',
        summary
    )

    # 7) "#Person1#ê³¼ #Person1#" â†’ "#Person1#ê³¼ #Person2#"
    summary = summary.replace("#Person1#ê³¼ #Person1#", "#Person1#ê³¼ #Person2#")
    summary = summary.replace("#Person1#ì™€ #Person1#", "#Person1#ì™€ #Person2#")

    # 8) "#Person1#ì€ #Person1#ê°€ / ëŠ” / ì„ ..." íŒ¨í„´ êµì •
    summary = summary.replace("#Person1#ì€ #Person1#ê°€", "#Person1#ì€ #Person2#ê°€")
    summary = summary.replace("#Person1#ì€ #Person1#ëŠ”", "#Person1#ì€ #Person2#ëŠ”")
    summary = summary.replace("#Person1#ì€ #Person1#ì„", "#Person1#ì€ #Person2#ì„")
    

    # 2) ë„ˆë¬´ ì¥í™©í•œ ê´„í˜¸/ëŒ€ê´„í˜¸ ì œê±° (í˜¹ì‹œ ë‚¨ì•„ ìˆë‹¤ë©´)
    summary = re.sub(r'\(.*?\)', '', summary)
    summary = re.sub(r'\[.*?\]', '', summary)

    # 9) Person + ì¡°ì‚¬ ì‚¬ì´ ë„ì–´ì“°ê¸° ì •ë¦¬
    summary = re.sub(
        r'(#Person\d+#)\s+(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—ê²Œ|ê»˜ì„œ|ê³¼|ì™€|ì˜|ë„|ë§Œ|ë¶€í„°|ê¹Œì§€|ì—|ì—ì„œ)',
        r'\1\2',
        summary
    )

    # 10) ê³µë°± ì •ë¦¬
    summary = re.sub(r'\s+', ' ', summary).strip()

    return summary


# ========================================
# 6. ë°°ì¹˜ Inference (ì™„ì „ ìˆ˜ì • ë²„ì „)
# ========================================

print("\nğŸš€ Inference ì‹œì‘...")
print(f"  Total: {len(test_df):,}ê°œ")
print(f"  Batch Size: {config['inference']['batch_size']}")
print(f"  Num Beams: {config['inference']['num_beams']}")

remove_tokens = config['inference']['remove_tokens']  # ë³€ìˆ˜ ì •ì˜
summaries = []

batch_size = config['inference']['batch_size']

for i in tqdm(range(0, len(test_df), batch_size), desc="Generating"):
    batch = test_df['dialogue_clean'].iloc[i:i+batch_size].tolist()
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(
        batch,
        max_length=config['tokenizer']['encoder_max_len'],
        padding=True,
        truncation=True,
        return_tensors='pt'
    ).to(device)
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=config['tokenizer']['decoder_max_len'],
            num_beams=config['inference']['num_beams'],
            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
            early_stopping=config['inference']['early_stopping'],
        )
    
    # ë””ì½”ë”©
    batch_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    
    # í›„ì²˜ë¦¬ (ì™„ì „ ìˆ˜ì •!)
    for summary in batch_summaries:
        # 1. remove_tokens ì œê±°
        for token in remove_tokens:
            summary = summary.replace(token, '')
        
        # 2. ì£¼ì–´ ë³´ì™„ (í•µì‹¬!)
        summary = fix_missing_subjects(summary)
        
        # 3. ìµœì¢… ì •ë¦¬
        summary = re.sub(r'\s+', ' ', summary).replace('\t', '').strip()
        summaries.append(summary)

print(f"âœ… Inference ì™„ë£Œ: {len(summaries)}ê°œ ìƒì„±")


# ========================================
# 7. ì œì¶œ íŒŒì¼ ìƒì„±
# ========================================

print("\nğŸ’¾ ì œì¶œ íŒŒì¼ ìƒì„±...")

# ë²„ì „ ì •ë³´
version = 'v2'
model_name = 'mbart'
date_str = datetime.now().strftime('%m%d')  # MMDD í˜•ì‹

# íŒŒì¼ëª… ìƒì„±
filename = f'submission_{version}_{model_name}_baseline_{date_str}.csv'

# sample_submission í˜•ì‹
submission = pd.DataFrame({
    'fname': [f'test_{i}' for i in range(len(summaries))],
    'summary': summaries
})

# ì €ì¥
os.makedirs('./predictions', exist_ok=True)
submission_path = os.path.join('./predictions', filename)
submission.to_csv(submission_path, index=False)

print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: {submission_path}")
print(f"   íŒŒì¼ëª…: {filename}")


# ========================================
# 8. ìƒ˜í”Œ í™•ì¸
# ========================================

print("\nğŸ“ ìƒì„± ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
print("=" * 60)

for i in range(min(3, len(test_df))):
    print(f"\n[{i+1}ë²ˆì§¸ ìƒ˜í”Œ]")
    print(f"ëŒ€í™”ë¬¸: {test_df['dialogue_clean'].iloc[i][:100]}...")
    print(f"ìš”ì•½ë¬¸: {summaries[i]}")
    print("-" * 60)

# ========================================
# 9. í†µê³„
# ========================================

print("\nğŸ“Š ìƒì„± ìš”ì•½ë¬¸ í†µê³„:")
summary_lengths = [len(s) for s in summaries]
print(f"  í‰ê·  ê¸¸ì´: {np.mean(summary_lengths):.1f}ì")
print(f"  ìµœì†Œ ê¸¸ì´: {np.min(summary_lengths)}ì")
print(f"  ìµœëŒ€ ê¸¸ì´: {np.max(summary_lengths)}ì")

# ========================================
# ì™„ë£Œ
# ========================================

print("\n" + "=" * 60)
print("âœ… inference.py ì™„ë£Œ!")
print("=" * 60)
print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
print(f"  - {submission_path}")
print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"  1. ì œì¶œ íŒŒì¼ í™•ì¸: cat {submission_path} | head")
print(f"  2. ëŒ€íšŒ ì‚¬ì´íŠ¸ì— ì œì¶œ!")
print("=" * 60)

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
