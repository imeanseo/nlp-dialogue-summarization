# ========================================
# v1_processing.py
# ì „ì²˜ë¦¬ + Config ìƒì„± (WandB ì œì™¸)
# ========================================

import pandas as pd
import numpy as np
import re
import os
import json
import yaml
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

print("=" * 60)
print("v1_processing.py - ë°ì´í„° ì „ì²˜ë¦¬")
print("=" * 60)

# ========================================
# 1. Config ì„¤ì • (í•™ìŠµìš©)
# ========================================

config_data = {
    'general': {
        'data_path': './',
        'model_name': 'facebook/mbart-large-50-many-to-many-mmt',
        'output_dir': './checkpoints',
    },
    
    'tokenizer': {
        'encoder_max_len': 400,
        'decoder_max_len': 80,
        'special_tokens': [],  # ë‚˜ì¤‘ì— ìë™ìœ¼ë¡œ ì±„ì›Œì§
    },
    
    'training': {
        'overwrite_output_dir': True,
        'num_train_epochs': 5,
        'learning_rate': 3e-5,
        'per_device_train_batch_size': 8,
        'per_device_eval_batch_size': 8,
        'warmup_ratio': 0.1,
        'weight_decay': 0.01,
        'lr_scheduler_type': 'cosine',
        'optim': 'adamw_torch',
        'gradient_accumulation_steps': 2,
        'save_steps': 500,
        'eval_steps': 500,
        'save_total_limit': 3,
        'fp16': True,
        'load_best_model_at_end': True,
        'seed': 42,
        'logging_dir': './logs',
        'logging_strategy': 'steps',
        'logging_steps': 100,
        'predict_with_generate': True,
        'generation_max_length': 80,
        'do_train': True,
        'do_eval': True,
        'early_stopping_patience': 3,
        'early_stopping_threshold': 0.001,
        'report_to': 'wandb',
    },
    
    'wandb': {
        'entity': 'imeanseo_',  # ìˆ˜ì •!
        'project': 'dialogue-summarization',
        'name': 'v1-mbart-baseline',
    },
    
    'inference': {
        'no_repeat_ngram_size': 2,
        'early_stopping': True,
        'generate_max_length': 80,
        'num_beams': 4,
        'batch_size': 32,
        'remove_tokens': ['<usr>', '</s>', '<s>', '<pad>'],
    },
}

print("âœ… Config ì„¤ì • ì™„ë£Œ")

# ========================================
# 2. ë°ì´í„° ë¡œë“œ
# ========================================

print("\nğŸ“‚ ë°ì´í„° ë¡œë”©...")

train_df = pd.read_csv('train.csv')
dev_df = pd.read_csv('dev.csv')
test_df = pd.read_csv('test.csv')

print(f"âœ… Train: {len(train_df):,}ê°œ")
print(f"âœ… Dev:   {len(dev_df):,}ê°œ")
print(f"âœ… Test:  {len(test_df):,}ê°œ")

# ========================================
# 3. ì „ì²˜ë¦¬ í•¨ìˆ˜
# ========================================

print("\nğŸ”§ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜...")

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if pd.isna(text):
        return ""
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text

def postprocess_text(text, remove_tokens):

    if pd.isna(text):
        return ""
    
    # Remove tokens
    for token in remove_tokens:
        text = text.replace(token, '')
    
    # Person + ì¡°ì‚¬ ë„ì–´ì“°ê¸° ì œê±°
    text = re.sub(
        r'(#Person\d+#)\s+(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—ê²Œ|ê»˜ì„œ|ê³¼|ì™€|ì˜|ë„|ë§Œ|ë¶€í„°|ê¹Œì§€|ì—|ì—ì„œ)',
        r'\1\2',
        text
    )
    
    text = text.replace('\t', '').replace('  ', ' ')
    text = text.strip()
    return text

# ì „ì²˜ë¦¬ ì ìš©
print("\nğŸ§¹ ì „ì²˜ë¦¬ ì ìš© ì¤‘...")

train_df['dialogue_clean'] = train_df['dialogue'].apply(clean_text)
train_df['summary_clean'] = train_df['summary'].apply(clean_text)
dev_df['dialogue_clean'] = dev_df['dialogue'].apply(clean_text)
dev_df['summary_clean'] = dev_df['summary'].apply(clean_text)
test_df['dialogue_clean'] = test_df['dialogue'].apply(clean_text)

print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")

# ìƒ˜í”Œ í™•ì¸
print("\nğŸ“ ì „ì²˜ë¦¬ ìƒ˜í”Œ:")
print(f"[ì›ë³¸] {train_df['dialogue'].iloc[0][:100]}...")
print(f"[ì²˜ë¦¬] {train_df['dialogue_clean'].iloc[0][:100]}...")

# ========================================
# 4. Special Tokens ì¶”ì¶œ
# ========================================

print("\nğŸ”§ Special Tokens ì¶”ì¶œ...")

def extract_special_tokens(dataframe):
    pattern = r'#\w+#'
    all_text = ' '.join(dataframe['dialogue'].astype(str))
    tokens = re.findall(pattern, all_text)
    return sorted(list(set(tokens)))

special_tokens = extract_special_tokens(train_df)

print(f"âœ… ë°œê²¬: {len(special_tokens)}ê°œ")
for i, token in enumerate(special_tokens[:10], 1):
    print(f"  {i}. {token}")

# Configì— ì €ì¥
config_data['tokenizer']['special_tokens'] = special_tokens

# ========================================
# 5. í† í¬ë‚˜ì´ì €ë¡œ ì‹¤ì œ ê¸¸ì´ í™•ì¸
# ========================================

print(f"\nğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë“œ...")

tokenizer = AutoTokenizer.from_pretrained(
    config_data['general']['model_name'],
    src_lang="ko_KR",
    tgt_lang="ko_KR"
)

# Special Token ì¶”ê°€
num_added = tokenizer.add_special_tokens({
    'additional_special_tokens': special_tokens
})
print(f"âœ… {num_added}ê°œ Special Token ì¶”ê°€")

# ìƒ˜í”Œë§
print("\nğŸ“Š í† í° ê¸¸ì´ ë¶„ì„ (ìƒ˜í”Œ 1000ê°œ)...")
sample_size = 1000
sample_indices = np.random.choice(len(train_df), min(sample_size, len(train_df)), replace=False)

dialogue_tokens = []
summary_tokens = []

for idx in sample_indices:
    d_tokens = tokenizer(train_df['dialogue_clean'].iloc[idx], truncation=False)
    dialogue_tokens.append(len(d_tokens['input_ids']))
    
    s_tokens = tokenizer(train_df['summary_clean'].iloc[idx], truncation=False)
    summary_tokens.append(len(s_tokens['input_ids']))

dialogue_tokens = np.array(dialogue_tokens)
summary_tokens = np.array(summary_tokens)

# í†µê³„
print("\n[ëŒ€í™”ë¬¸ í† í° ê¸¸ì´]")
print(f"  í‰ê· :     {dialogue_tokens.mean():.1f}")
print(f"  ì¤‘ê°„ê°’:   {np.median(dialogue_tokens):.0f}")
print(f"  95%:      {np.percentile(dialogue_tokens, 95):.0f}")
print(f"  ìµœëŒ€:     {dialogue_tokens.max()}")

print("\n[ìš”ì•½ë¬¸ í† í° ê¸¸ì´]")
print(f"  í‰ê· :     {summary_tokens.mean():.1f}")
print(f"  ì¤‘ê°„ê°’:   {np.median(summary_tokens):.0f}")
print(f"  95%:      {np.percentile(summary_tokens, 95):.0f}")
print(f"  ìµœëŒ€:     {summary_tokens.max()}")

print(f"\nğŸ“‹ Config ì„¤ì •ê°’:")
print(f"  encoder_max_len: {config_data['tokenizer']['encoder_max_len']}")
print(f"  decoder_max_len: {config_data['tokenizer']['decoder_max_len']}")

# ========================================
# 6. ì €ì¥
# ========================================

print("\nğŸ’¾ ì €ì¥ ì¤‘...")

os.makedirs('./processed_data', exist_ok=True)

# ì „ì²˜ë¦¬ëœ ë°ì´í„°
train_df.to_csv('./processed_data/train_processed.csv', index=False)
dev_df.to_csv('./processed_data/dev_processed.csv', index=False)
test_df.to_csv('./processed_data/test_processed.csv', index=False)

# Config ì €ì¥ (YAML & JSON ë‘˜ ë‹¤)
with open('./config.yaml', 'w', encoding='utf-8') as f:
    yaml.dump(config_data, f, allow_unicode=True, default_flow_style=False)

with open('./processed_data/config.json', 'w', encoding='utf-8') as f:
    json.dump(config_data, f, ensure_ascii=False, indent=2)

# Special tokens ë³„ë„ ì €ì¥
with open('./processed_data/special_tokens.json', 'w', encoding='utf-8') as f:
    json.dump(special_tokens, f, ensure_ascii=False, indent=2)

print("âœ… ì €ì¥ ì™„ë£Œ:")
print("  - ./config.yaml")
print("  - ./processed_data/train_processed.csv")
print("  - ./processed_data/dev_processed.csv")
print("  - ./processed_data/test_processed.csv")
print("  - ./processed_data/config.json")
print("  - ./processed_data/special_tokens.json")

# ========================================
# 7. ì‹œê°í™”
# ========================================

print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ëŒ€í™”ë¬¸
axes[0].hist(dialogue_tokens, bins=50, color='skyblue', edgecolor='black', alpha=0.7)
axes[0].axvline(dialogue_tokens.mean(), color='red', linestyle='--', 
                label=f'Mean: {dialogue_tokens.mean():.0f}')
axes[0].axvline(config_data['tokenizer']['encoder_max_len'], 
                color='orange', linestyle='--', linewidth=2,
                label=f"Config: {config_data['tokenizer']['encoder_max_len']}")
axes[0].set_title('Dialogue Token Length Distribution')
axes[0].set_xlabel('Token Length')
axes[0].set_ylabel('Frequency')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ìš”ì•½ë¬¸
axes[1].hist(summary_tokens, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)
axes[1].axvline(summary_tokens.mean(), color='red', linestyle='--',
                label=f'Mean: {summary_tokens.mean():.0f}')
axes[1].axvline(config_data['tokenizer']['decoder_max_len'],
                color='orange', linestyle='--', linewidth=2,
                label=f"Config: {config_data['tokenizer']['decoder_max_len']}")
axes[1].set_title('Summary Token Length Distribution')
axes[1].set_xlabel('Token Length')
axes[1].set_ylabel('Frequency')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('./processed_data/token_distribution.png', dpi=300, bbox_inches='tight')
print("âœ… ì‹œê°í™” ì €ì¥: ./processed_data/token_distribution.png")
plt.close()

# ========================================
# ì™„ë£Œ
# ========================================

print("\n" + "=" * 60)
print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("=" * 60)
print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
print("  1. config.yaml               - í•™ìŠµ ì„¤ì • (í•™ìŠµ ì‹œ ì‚¬ìš©)")
print("  2. processed_data/           - ì „ì²˜ë¦¬ëœ ë°ì´í„°")
print("  3. token_distribution.png    - ì‹œê°í™”")
