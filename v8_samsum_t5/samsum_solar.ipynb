{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b838d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /data/ephemeral/home/.cache/kagglehub/datasets/nileshmalode1/samsum-dataset-text-summarization/versions/1\n",
      "['samsum-test.csv', 'samsum-train.csv', 'samsum-validation.csv', 'samsum_dataset']\n",
      "Index(['id', 'dialogue', 'summary'], dtype='object')\n",
      "         id                                           dialogue  \\\n",
      "0  13818513  Amanda: I baked  cookies. Do you want some?\\r\\...   \n",
      "1  13728867  Olivia: Who are you voting for in this electio...   \n",
      "2  13681000  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...   \n",
      "3  13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
      "4  13728094  Sam: hey  overheard rick say something\\r\\nSam:...   \n",
      "\n",
      "                                             summary  \n",
      "0  Amanda baked cookies and will bring Jerry some...  \n",
      "1  Olivia and Olivier are voting for liberals in ...  \n",
      "2  Kim may try the pomodoro technique recommended...  \n",
      "3  Edward thinks he is in love with Bella. Rachel...  \n",
      "4  Sam is confused, because he overheard Rick com...  \n"
     ]
    }
   ],
   "source": [
    "# !pip install kagglehub\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1) SamSum ë‹¤ìš´ë¡œë“œ\n",
    "path = kagglehub.dataset_download(\"nileshmalode1/samsum-dataset-text-summarization\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(os.listdir(path))\n",
    "\n",
    "train_raw = pd.read_csv(os.path.join(path, \"samsum-train.csv\"))\n",
    "valid_raw = pd.read_csv(os.path.join(path, \"samsum-validation.csv\"))\n",
    "test_raw  = pd.read_csv(os.path.join(path, \"samsum-test.csv\"))\n",
    "\n",
    "print(train_raw.columns)\n",
    "print(train_raw.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2aba64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ:\n",
      "/root/nlp_data/samsum/samsum_train_en.csv\n",
      "/root/nlp_data/samsum/samsum_valid_en.csv\n",
      "/root/nlp_data/samsum/samsum_test_en.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base = \"/data/ephemeral/home/.cache/kagglehub/datasets/nileshmalode1/samsum-dataset-text-summarization/versions/1\"\n",
    "out_dir = \"/root/nlp_data/samsum\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "train_raw = pd.read_csv(os.path.join(base, \"samsum-train.csv\"))\n",
    "valid_raw = pd.read_csv(os.path.join(base, \"samsum-validation.csv\"))\n",
    "test_raw  = pd.read_csv(os.path.join(base, \"samsum-test.csv\"))\n",
    "\n",
    "# id, dialogue, summary ì¤‘ì—ì„œ dialogue/summaryë§Œ ì“°ê³  ì´ë¦„ í†µì¼\n",
    "train_df = train_raw.rename(columns={\"dialogue\": \"dialogue_en\", \"summary\": \"summary_en\"})[[\"dialogue_en\", \"summary_en\"]]\n",
    "valid_df = valid_raw.rename(columns={\"dialogue\": \"dialogue_en\", \"summary\": \"summary_en\"})[[\"dialogue_en\", \"summary_en\"]]\n",
    "test_df  = test_raw.rename(columns={\"dialogue\": \"dialogue_en\", \"summary\": \"summary_en\"})[[\"dialogue_en\", \"summary_en\"]]\n",
    "\n",
    "train_path = os.path.join(out_dir, \"samsum_train_en.csv\")\n",
    "valid_path = os.path.join(out_dir, \"samsum_valid_en.csv\")\n",
    "test_path  = os.path.join(out_dir, \"samsum_test_en.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "valid_df.to_csv(valid_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"âœ… ì €ì¥ ì™„ë£Œ:\")\n",
    "print(train_path)\n",
    "print(valid_path)\n",
    "print(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a1b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLAR_API_KEY: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Solar API ë²ˆì—­ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"SOLAR_API_KEY\"] = \"up\"\n",
    "\n",
    "SOLAR_API_KEY = os.getenv(\"SOLAR_API_KEY\")\n",
    "SOLAR_URL = \"https://api.upstage.ai/v1/solar/chat/completions\"\n",
    "print(\"SOLAR_API_KEY:\", SOLAR_API_KEY is not None)\n",
    "\n",
    "# MODEL_NAME = \"solar-pro2\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {SOLAR_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "def call_solar(prompt: str, max_tokens: int = 512, model: str = \"solar-1-mini-chat\"):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator for English and Korean.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    resp = requests.post(SOLAR_URL, headers=HEADERS, json=payload, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "def build_dialogue_prompt(text_en: str) -> str:\n",
    "    example_ko_dialogue = \"\"\"#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
    "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
    "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
    "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ë‹¤ìŒì€ ì´ ëŒ€íšŒì—ì„œ ì‚¬ìš©í•˜ëŠ” í•œêµ­ì–´ ëŒ€í™” í˜•ì‹ì˜ ê·œì¹™ì´ì•¼.\n",
    "\n",
    "[ëŒ€í™” ìŠ¤íƒ€ì¼ ê·œì¹™]\n",
    "- ê° ì¤„ì€ \"#Person1#:\", \"#Person2#:\" ë¡œ ì‹œì‘.\n",
    "- í™”ìë§ˆë‹¤ í•œ ì¤„ì”©, ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„.\n",
    "- ë§íˆ¬ëŠ” ì¼ìƒì ì¸ êµ¬ì–´ì²´ ì¡´ëŒ“ë§ ìœ„ì£¼.\n",
    "- ì˜ì–´ ê³ ìœ ëª…ì‚¬ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­.\n",
    "\n",
    "[í•œêµ­ì–´ ëŒ€í™” ì˜ˆì‹œ]\n",
    "{example_ko_dialogue}\n",
    "\n",
    "ìœ„ ê·œì¹™ê³¼ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´ì„œ, ì•„ë˜ ì˜ì–´ ëŒ€í™”ë¥¼\n",
    "ë™ì¼í•œ í˜•ì‹ì˜ í•œêµ­ì–´ ëŒ€í™”ë¡œ ë²ˆì—­í•´ì¤˜.\n",
    "\n",
    "[ì˜ì–´ ëŒ€í™”]\n",
    "{text_en}\n",
    "\n",
    "[ì¶œë ¥ í˜•ì‹]\n",
    "- ê° ì¤„ì„ \"#Person1#:\" ë˜ëŠ” \"#Person2#:\" ë¡œ ì‹œì‘.\n",
    "- ì¶”ê°€ ì„¤ëª… ì—†ì´ ëŒ€í™”ë¬¸ë§Œ ì¶œë ¥.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_summary_prompt(text_en: str) -> str:\n",
    "    # ì‹¤ì œ train.csvì—ì„œ ë½‘ì€ ì˜ˆì‹œ\n",
    "    example_ko_summary = (\n",
    "        \"#Person1#ì€ #Person2#ì—ê²Œ ê±´ê°•ì„ ìœ„í•´ ë‹´ë°°ë¥¼ ëŠì„ ê²ƒì„ ê¶Œìœ í•˜ê³ , \"\n",
    "        \"#Person2#ì€ ì´ë¥¼ ì–´ë µë‹¤ê³  ëŠë¼ì§€ë§Œ ë™ì˜í•©ë‹ˆë‹¤.\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ë‹¤ìŒì€ í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìš”ì•½ë¬¸ì˜ ìŠ¤íƒ€ì¼ ì„¤ëª…ì´ì•¼.\n",
    "\n",
    "[ìš”ì•½ ìŠ¤íƒ€ì¼ ê·œì¹™]\n",
    "- ê¸¸ì´: 1~2ë¬¸ì¥.\n",
    "- í˜•ì‹: í™”ìëŠ” \"#Person1#\", \"#Person2#\"ì™€ ê°™ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©.\n",
    "- ë‚´ìš©: ëŒ€í™”ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í–‰ë™ê³¼ ê²°ì •ì„ ìš”ì•½.\n",
    "  - ì˜ˆ: ëˆ„ê°€ ë¬´ì—‡ì„ ìš”ì²­/ì„¤ëª…/ì œì•ˆ/ë™ì˜/ê±°ì ˆí–ˆëŠ”ì§€.\n",
    "- í†¤: ì¡´ëŒ“ë§, ì„¤ëª…ì²´. ê°ì • ë¬˜ì‚¬ëŠ” ìµœì†Œí•œìœ¼ë¡œë§Œ ì‚¬ìš©.\n",
    "- ëŒ€í™”ì˜ ì„¸ë¶€ ëŒ€ì‚¬ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³ , í•œ ë‹¨ê³„ ìœ„ì—ì„œ ìƒí™©ì„ ì •ë¦¬.\n",
    "\n",
    "[í•œêµ­ì–´ ìš”ì•½ ì˜ˆì‹œ]\n",
    "{example_ko_summary}\n",
    "\n",
    "ìœ„ ê·œì¹™ê³¼ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´ì„œ, ì•„ë˜ ì˜ì–´ ìš”ì•½ ë¬¸ì¥ì„\n",
    "ìœ„ì™€ ê°™ì€ ìŠ¤íƒ€ì¼ì˜ í•œêµ­ì–´ ìš”ì•½ìœ¼ë¡œ ë°”ê¿”ì¤˜.\n",
    "\n",
    "[ì˜ì–´ ìš”ì•½]\n",
    "{text_en}\n",
    "\n",
    "[ì¶œë ¥ í˜•ì‹]\n",
    "- 1~2ë¬¸ì¥.\n",
    "- \"#Person1#\", \"#Person2#\" í‘œê¸°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©.\n",
    "- ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë©”íƒ€ ì½”ë©˜íŠ¸ ì—†ì´ ìš”ì•½ë¬¸ë§Œ ì¶œë ¥.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ad0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_file(in_path: str, out_path: str, batch_size: int = 4, sleep_sec: float = 0.5):\n",
    "    print(f\"ğŸ“‚ ì…ë ¥: {in_path}\")\n",
    "    df = pd.read_csv(in_path)\n",
    "    \n",
    "    dialogues_ko = []\n",
    "    summaries_ko = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=f\"Translating {os.path.basename(in_path)}\"):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        for _, row in batch.iterrows():\n",
    "            d_en = str(row[\"dialogue_en\"])\n",
    "            s_en = str(row[\"summary_en\"])\n",
    "            \n",
    "            d_ko = call_solar(build_dialogue_prompt(d_en), max_tokens=512)\n",
    "            s_ko = call_solar(build_summary_prompt(s_en), max_tokens=128)\n",
    "            \n",
    "            dialogues_ko.append(d_ko)\n",
    "            summaries_ko.append(s_ko)\n",
    "            \n",
    "            time.sleep(sleep_sec)\n",
    "    \n",
    "    df[\"dialogue_ko\"] = dialogues_ko\n",
    "    df[\"summary_ko\"] = summaries_ko\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"âœ… ë²ˆì—­ ì™„ë£Œ â†’ {out_path}\")\n",
    "    display(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d912d8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì˜ì–´ ëŒ€í™” ===\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-) \n",
      "\n",
      "=== ë²ˆì—­ëœ í•œêµ­ì–´ ëŒ€í™” ===\n",
      "#Person1#: ë‚´ê°€ ì¿ í‚¤ êµ¬ì› ì–´. ë¨¹ì„ë˜?\n",
      "#Person2#: ì¢‹ì•„!\n",
      "#Person1#: ë‚´ì¼ ê°€ì ¸ë‹¤ ì¤„ê²Œ :-) \n",
      "\n",
      "=== ë²ˆì—­ëœ í•œêµ­ì–´ ìš”ì•½ ===\n",
      "#Person1#ì€ ì˜¤ëŠ˜ ì¿ í‚¤ë¥¼ êµ¬ì› ê³ , ë‚´ì¼ #Person2#ì—ê²Œ ê·¸ ì¿ í‚¤ë¥¼ ê°€ì ¸ë‹¤ì£¼ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "SAMSUM_DIR = \"/root/nlp_data/samsum\"  # ì—¬ê¸°ë¡œ ê³ ì •\n",
    "\n",
    "samsum_train_en = os.path.join(SAMSUM_DIR, \"samsum_train_en.csv\")\n",
    "df_sample = pd.read_csv(samsum_train_en).head(1)\n",
    "\n",
    "text_dialogue_en = df_sample[\"dialogue_en\"].iloc[0]\n",
    "text_summary_en = df_sample[\"summary_en\"].iloc[0]\n",
    "\n",
    "print(\"=== ì˜ì–´ ëŒ€í™” ===\")\n",
    "print(text_dialogue_en[:300], \"\\n\")\n",
    "\n",
    "ko_dialogue = call_solar(build_dialogue_prompt(text_dialogue_en), max_tokens=512)\n",
    "print(\"=== ë²ˆì—­ëœ í•œêµ­ì–´ ëŒ€í™” ===\")\n",
    "print(ko_dialogue, \"\\n\")\n",
    "\n",
    "ko_summary = call_solar(build_summary_prompt(text_summary_en), max_tokens=128)\n",
    "print(\"=== ë²ˆì—­ëœ í•œêµ­ì–´ ìš”ì•½ ===\")\n",
    "print(ko_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2587be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ì…ë ¥: /root/nlp_data/samsum/samsum_valid_en.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating samsum_valid_en.csv:   0%|          | 0/205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating samsum_valid_en.csv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [45:39<00:00, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë²ˆì—­ ì™„ë£Œ â†’ /root/nlp_data/samsum/samsum_valid_ko.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_en</th>\n",
       "      <th>summary_en</th>\n",
       "      <th>dialogue_ko</th>\n",
       "      <th>summary_ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A: Hi Tom, are you busy tomorrowâ€™s afternoon?\\...</td>\n",
       "      <td>A will go to the animal shelter tomorrow to ge...</td>\n",
       "      <td>#Person1#: ì•ˆë…• Tom, ë‚´ì¼ ì˜¤í›„ì— ì‹œê°„ ìˆì–´?\\n#Person2#: ê¸€...</td>\n",
       "      <td>#Person1#ì€ ì•„ë“¤ì„ ìœ„í•´ ë‚´ì¼ ë™ë¬¼ ë³´í˜¸ì†Œì— ê°€ì„œ ê°•ì•„ì§€ë¥¼ ì…ì–‘í•˜ê¸°ë¡œ ê²°ì •í–ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emma: Iâ€™ve just fallen in love with this adven...</td>\n",
       "      <td>Emma and Rob love the advent calendar. Lauren ...</td>\n",
       "      <td>#Emma#: ì´ ì–´ë“œë²¤íŠ¸ ìº˜ë¦°ë” ë„ˆë¬´ ë§˜ì— ë“¤ì–´! ìµœê³ ì•¼! ìš°ë¦¬ ì• ë“¤ë„ í•˜ë‚˜ ì‚¬ì£¼...</td>\n",
       "      <td>#Person1#ê³¼ #Person2#ëŠ” ì–´ë“œë²¤íŠ¸ ìº˜ë¦°ë”ë¥¼ ì¢‹ì•„í•˜ë©°, #Person3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         dialogue_en  \\\n",
       "0  A: Hi Tom, are you busy tomorrowâ€™s afternoon?\\...   \n",
       "1  Emma: Iâ€™ve just fallen in love with this adven...   \n",
       "\n",
       "                                          summary_en  \\\n",
       "0  A will go to the animal shelter tomorrow to ge...   \n",
       "1  Emma and Rob love the advent calendar. Lauren ...   \n",
       "\n",
       "                                         dialogue_ko  \\\n",
       "0  #Person1#: ì•ˆë…• Tom, ë‚´ì¼ ì˜¤í›„ì— ì‹œê°„ ìˆì–´?\\n#Person2#: ê¸€...   \n",
       "1  #Emma#: ì´ ì–´ë“œë²¤íŠ¸ ìº˜ë¦°ë” ë„ˆë¬´ ë§˜ì— ë“¤ì–´! ìµœê³ ì•¼! ìš°ë¦¬ ì• ë“¤ë„ í•˜ë‚˜ ì‚¬ì£¼...   \n",
       "\n",
       "                                          summary_ko  \n",
       "0  #Person1#ì€ ì•„ë“¤ì„ ìœ„í•´ ë‚´ì¼ ë™ë¬¼ ë³´í˜¸ì†Œì— ê°€ì„œ ê°•ì•„ì§€ë¥¼ ì…ì–‘í•˜ê¸°ë¡œ ê²°ì •í–ˆ...  \n",
       "1  #Person1#ê³¼ #Person2#ëŠ” ì–´ë“œë²¤íŠ¸ ìº˜ë¦°ë”ë¥¼ ì¢‹ì•„í•˜ë©°, #Person3...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SAMSUM_DIR = \"/root/nlp_data/samsum\"\n",
    "\n",
    "# samsum_train_en = os.path.join(SAMSUM_DIR, \"samsum_train_en.csv\")\n",
    "# samsum_valid_en = os.path.join(SAMSUM_DIR, \"samsum_valid_en.csv\")\n",
    "\n",
    "# ğŸ“Œ train ì¼ë¶€ë§Œ ë½‘ì•„ì„œ ë²ˆì—­\n",
    "# train_df_sample = pd.read_csv(samsum_train_en).head(500)  # 500ê°œë§Œ\n",
    "# train_df_sample.to_csv(os.path.join(SAMSUM_DIR, \"samsum_train_en_500.csv\"), index=False)\n",
    "\n",
    "# samsum_train_en_500 = os.path.join(SAMSUM_DIR, \"samsum_train_en_500.csv\")\n",
    "# samsum_train_ko_500 = os.path.join(SAMSUM_DIR, \"samsum_train_ko_500.csv\")\n",
    "# samsum_valid_ko = os.path.join(SAMSUM_DIR, \"samsum_valid_ko.csv\")\n",
    "\n",
    "# translate_file(samsum_train_en_500, samsum_train_ko_500, batch_size=4, sleep_sec=0.5)\n",
    "# translate_file(samsum_valid_en, samsum_valid_ko, batch_size=4, sleep_sec=0.5)\n",
    "SAMSUM_DIR = \"/root/nlp_data/samsum\"\n",
    "samsum_valid_en = os.path.join(SAMSUM_DIR, \"samsum_valid_en.csv\")\n",
    "samsum_valid_ko = os.path.join(SAMSUM_DIR, \"samsum_valid_ko.csv\")\n",
    "\n",
    "translate_file(samsum_valid_en, samsum_valid_ko, batch_size=4, sleep_sec=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d26b4e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 500ê°œ\n",
      "Valid: 818ê°œ\n",
      "ìƒ˜í”Œ:\n",
      "                                         dialogue_ko  \\\n",
      "0  #Person1#: ë‚´ê°€ ì¿ í‚¤ êµ¬ì› ì–´. ë¨¹ì„ë˜?\\n#Person2#: ì¢‹ì•„!\\n#P...   \n",
      "1  #Person1#: ì´ë²ˆ ì„ ê±°ì—ì„œ ëˆ„êµ¬ ë½‘ì„ ê±°ì•¼? \\n#Person2#: ì–¸ì œë‚˜ì²˜...   \n",
      "\n",
      "                                          summary_ko  \n",
      "0  #Person1#ì€ ì˜¤ëŠ˜ ì¿ í‚¤ë¥¼ êµ¬ì› ê³ , ë‚´ì¼ #Person2#ì—ê²Œ ê·¸ ì¿ í‚¤ë¥¼ ê°€ì ¸...  \n",
      "1  #Person1#ê³¼ #Person2#ëŠ” ì´ë²ˆ ì„ ê±°ì—ì„œ ììœ ì£¼ì˜ìë“¤ì—ê²Œ íˆ¬í‘œí•˜ê¸°ë¡œ ê²°...  \n"
     ]
    }
   ],
   "source": [
    "# ìƒˆ ë…¸íŠ¸ë¶ì—ì„œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥\n",
    "import pandas as pd\n",
    "\n",
    "train_ko = pd.read_csv(\"/root/nlp_data/samsum/samsum_train_ko_500.csv\")\n",
    "valid_ko = pd.read_csv(\"/root/nlp_data/samsum/samsum_valid_ko.csv\")\n",
    "\n",
    "print(f\"Train: {len(train_ko)}ê°œ\")\n",
    "print(f\"Valid: {len(valid_ko)}ê°œ\")\n",
    "print(\"ìƒ˜í”Œ:\")\n",
    "print(train_ko[[\"dialogue_ko\", \"summary_ko\"]].head(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
