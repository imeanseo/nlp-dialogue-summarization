import os
import yaml
import pandas as pd
import torch
from tqdm import tqdm
from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

print("=" * 80)
print("v5_inference.py - T5-Large ì¶”ë¡ ")
print("=" * 80)

# Config ë¡œë“œ
with open('./v5_t5_large/v5_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

print(f"\nâœ… Config ë¡œë“œ ì™„ë£Œ")

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
TEST_MODE = False # Falseë¡œ ë°”ê¾¸ë©´ ì „ì²´ 500ê°œ ì²˜ë¦¬
TEST_SAMPLES = 10

# ëª¨ë¸ & Tokenizer ë¡œë“œ
model_path = os.path.join(config['general']['output_dir'], "final_model")
print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
print(f"  ê²½ë¡œ: {model_path}")

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.eval()

print(f"  Device: {device}")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
test_df = pd.read_csv(os.path.join(config['general']['data_path'], 'test.csv'))

if TEST_MODE:
    test_df = test_df.head(TEST_SAMPLES)
    print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {TEST_SAMPLES}ê°œë§Œ ì¶”ë¡ ")

print(f"âœ… Test ë°ì´í„°: {len(test_df)}ê°œ")

# ì¶”ë¡  í•¨ìˆ˜
def generate_summary(dialogue, max_length=120, num_beams=4):
    """
    T5ë¡œ ìš”ì•½ ìƒì„±
    """
    # T5 prefix ì¶”ê°€
    input_text = "summarize: " + dialogue
    
    # Tokenize
    inputs = tokenizer(
        input_text,
        max_length=config['tokenizer']['encoder_max_len'],
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    ).to(device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            max_length=max_length,
            num_beams=num_beams,
            early_stopping=True,
            no_repeat_ngram_size=3,
            length_penalty=1.0
        )
    
    # Decode
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

# ì¶”ë¡  ì‹¤í–‰
print(f"\nğŸ”® ì¶”ë¡  ì‹œì‘...")
summaries = []

for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Generating"):
    dialogue = row['dialogue']
    summary = generate_summary(dialogue)
    summaries.append(summary)
    
    # ì²˜ìŒ 3ê°œ ì¶œë ¥
    if idx < 3:
        print(f"\n[{idx}] {row['fname']}")
        print(f"  ëŒ€í™”: {dialogue[:100]}...")
        print(f"  ìš”ì•½: {summary}")


from datetime import datetime
import pytz  # ì‹œê°„ëŒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ê²°ê³¼ ì €ì¥
kst = pytz.timezone('Asia/Seoul')
date_str = datetime.now(kst).strftime('%m%d_%H%M') 
output_filename = f"submission_samsum_{'test' if TEST_MODE else 'full'}_{date_str}.csv"
submission = pd.DataFrame({
    'fname': test_df['fname'],
    'summary': summaries
})
submission.to_csv(output_filename, index=False, encoding='utf-8-sig')
os.makedirs('./predictions', exist_ok=True)
submission.to_csv(f'./predictions/{output_filename}', index=False)

print(f"\nâœ… ì¶”ë¡  ì™„ë£Œ!")
print(f"  ì €ì¥ íŒŒì¼: {output_filename}")
print(f"  ì´ {len(summaries)}ê°œ ìš”ì•½ ìƒì„±")

# ìµœì¢… í™•ì¸
print(f"\nğŸ“‹ ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:")
print(submission.head(10))
