general:
  data_path: ./
  model_name: lcw99/t5-large-korean-text-summary
  output_dir: ./checkpoints_v6_t5
  cache_dir: ./cache

tokenizer:
  encoder_max_len: 512  # 1000→512 (dialogue 평균 200~300자)
  decoder_max_len: 100  # 120→100 (요약 짧게)

training:
  num_train_epochs: 2  # 5→2 (overfitting 방지)
  per_device_train_batch_size: 2  # 1→2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8  # effective=16
  
  learning_rate: 0.00005  # 3e-5→5e-5 (좀 더 공격적)
  warmup_steps: 200  # 100→200
  weight_decay: 0.01
  
  save_strategy: epoch
  evaluation_strategy: epoch
  save_total_limit: 2  # 3→2
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  
  logging_dir: ./logs_v6
  logging_steps: 50
  report_to: none  # wandb 끄기 (속도↑)
  
  fp16: true
  gradient_checkpointing: true
  dataloader_num_workers: 2  # 4→2
  
  predict_with_generate: true
  generation_max_length: 100
  generation_num_beams: 6  # 4→6 (품질↑)

wandb:
  entity: imeanseo_
  project: dialogue-summarization
  name: v6-t5-large-optimized
